{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import sys\n",
    "from random import shuffle\n",
    "from random import uniform\n",
    "import zipfile\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_train_path = '/data2/cityscapes_dataset/gtFine/train'\n",
    "imgs_train_path = '/data2/cityscapes_dataset/leftImg8bit/train'\n",
    "gt_val_path = '/data2/cityscapes_dataset/gtFine/val'\n",
    "imgs_val_path = '/data2/cityscapes_dataset/leftImg8bit/val'\n",
    "gt_test_path = '/data2/cityscapes_dataset/gtFine/test'\n",
    "imgs_test_path = '/data2/cityscapes_dataset/leftImg8bit/test'\n",
    "\n",
    "# Get filenames of training data and gt, specific for cityscapes dataset\n",
    "def get_files(imgs_dir, gt_dir):\n",
    "    \n",
    "    cities = os.listdir(imgs_dir)\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    for city in cities:\n",
    "        new_gt_path = os.path.join(gt_dir, city)\n",
    "        new_imgs_path = os.path.join(imgs_dir, city)\n",
    "        gt += glob.glob(os.path.join(new_gt_path, \"*labelIds.png\"))\n",
    "        imgs += glob.glob(os.path.join(new_imgs_path, \"*.png\"))\n",
    "    imgs.sort()\n",
    "    gt.sort()\n",
    "    return imgs, gt\n",
    "\n",
    "# Get filenames of training data and gt\n",
    "train_imgs, train_gt = get_files(imgs_train_path, gt_train_path)\n",
    "val_imgs, val_gt = get_files(imgs_val_path, gt_val_path)\n",
    "test_imgs, test_gt = get_files(imgs_test_path, gt_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image_path=None, img=None, from_path=True):\n",
    "    if (from_path == True):\n",
    "        img = misc.imread(image_path)\n",
    "    if len(img.shape) == 4:\n",
    "        img = np.squeeze(img)\n",
    "    if img.dtype != np.uint8:\n",
    "        img = img.astype(np.uint8)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare_ground_truth for cityscape data\n",
    "def prepare_ground_truth(img):\n",
    "    \n",
    "    # Five classes: road, side_walk, pedestrian, vehicles, others\n",
    "    NUM_CLASSES = 5\n",
    "    new_image = np.zeros((img.shape[0], img.shape[1], NUM_CLASSES))\n",
    "    \n",
    "    # (original_id)\n",
    "    # road\n",
    "    road_mask = img == 7\n",
    "    # sidewalk \n",
    "    side_mask = img == 8\n",
    "    # pedestrians[person,rider]\n",
    "    ped_mask = np.logical_or(img == 24, img == 25)\n",
    "    # vehicles[car,truck,bus,caravan,trailer,train,motorcycle, bicycle, license plate]\n",
    "    car_mask = np.logical_or.reduce((img == 26, img == 27, img == 28,\n",
    "                                      img == 29, img == 30, img == 31,\n",
    "                                      img == 32, img == 33, img == -1))\n",
    "    # everything else\n",
    "    else_mask = np.logical_not(np.logical_or.reduce((road_mask, side_mask,\n",
    "                                                     ped_mask, car_mask)))\n",
    "    \n",
    "    new_image[:,:,0] = road_mask\n",
    "    new_image[:,:,1] = side_mask\n",
    "    new_image[:,:,2] = ped_mask\n",
    "    new_image[:,:,3] = car_mask\n",
    "    new_image[:,:,4] = else_mask\n",
    "    \n",
    "    return new_image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-f87b9657006a>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f87b9657006a>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    building_mask = img == 11v\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# num_classes = 20, 19 objects classes plus one background glass\n",
    "'''\n",
    "def prepare_ground_truth_normal(img):\n",
    "    \n",
    "    # Five classes: road, side_walk, pedestrian, vehicles, others\n",
    "    NUM_CLASSES = 20\n",
    "    new_image = np.zeros((img.shape[0], img.shape[1], NUM_CLASSES))\n",
    "    \n",
    "    # (original_id)\n",
    "    # road\n",
    "    road_mask = img == 7\n",
    "    # sidewalk \n",
    "    sidewalk_mask = img == 8\n",
    "    \n",
    "    building_mask = img == 11\n",
    "    wall_mask = img == 12\n",
    "    fence_mask = img == 13\n",
    "    pole_mask = img == 17\n",
    "    traffic_light_mask = img == 19\n",
    "    traffic_sign_mask = img == 20\n",
    "    vegetation_mask = img == 21\n",
    "    terrain_mask = img == 22\n",
    "    sky_mask = img == 23\n",
    "    person_mask = img == 24\n",
    "    rider_mask = img == 25\n",
    "    car_mask = img == 26\n",
    "    truck_mask = img == 27\n",
    "    bus_mask = img == 28\n",
    "    train_mask = img == 31\n",
    "    motorcycle_mask = img == 32\n",
    "    bicycle_mask = img == 33\n",
    "    \n",
    "    # everything else\n",
    "    else_mask = np.logical_not(np.logical_or.reduce((road_mask, sidewalk_mask, building_mask, wall_mask,\n",
    "                                                     fence_mask, pole_mask, traffic_light_mask, traffic_sign_mask,\n",
    "                                                     vegetation_mask, terrain_mask, sky_mask, person_mask,\n",
    "                                                     rider_mask, car_mask, truck_mask, bus_mask,\n",
    "                                                     train_mask, motorcycle_mask, bicycle_mask)))\n",
    "\n",
    "\n",
    "    new_image[:,:,0] = road_mask\n",
    "    new_image[:,:,1] = sidewalk_mask\n",
    "    new_image[:,:,2] = building_mask\n",
    "    new_image[:,:,3] = wall_mask\n",
    "    new_image[:,:,4] = fence_mask\n",
    "    new_image[:,:,5] = pole_mask\n",
    "    new_image[:,:,6] = traffic_light_mask\n",
    "    new_image[:,:,7] = traffic_sign_mask\n",
    "    new_image[:,:,8] = vegetation_mask\n",
    "    new_image[:,:,9] = terrain_mask\n",
    "    new_image[:,:,10] = sky_mask\n",
    "    new_image[:,:,11] = person_mask\n",
    "    new_image[:,:,12] = rider_mask\n",
    "    new_image[:,:,13] = car_mask\n",
    "    new_image[:,:,14] = truck_mask\n",
    "    new_image[:,:,15] = bus_mask\n",
    "    new_image[:,:,16] = train_mask\n",
    "    new_image[:,:,17] = motorcycle_mask\n",
    "    new_image[:,:,18] = bicycle_mask\n",
    "    new_image[:,:,19] = else_mask\n",
    "    \n",
    "    return new_image.astype(np.float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(batch_size=1, num_classes=5, mode='train', imgs=train_imgs, gt=train_gt, im_size=500):\n",
    "    \n",
    "    # Expects sorted lists of training images and ground truth as\n",
    "    # 'data' and 'labels'\n",
    "    if (mode == 'val'):\n",
    "        imgs = val_imgs\n",
    "        gt = val_gt\n",
    "    elif (mode == 'test'):\n",
    "        imgs = test_imgs\n",
    "        gt = test_gt\n",
    "    \n",
    "    # get shape from any image\n",
    "    #shape_im = misc.imread(random_im_path)\n",
    "    \n",
    "    # Shuffle training dataset\n",
    "    #if mode=='train':\n",
    "    combined = list(zip(imgs, gt))  \n",
    "    shuffle(combined)\n",
    "    imgs[:], gt[:] = zip(*combined)\n",
    "    \n",
    "    while(True):\n",
    "        for i in range(0,len(imgs),batch_size):\n",
    "            #images = np.empty((batch_size, im_size, im_size, shape_im.shape[2]))\n",
    "            images = np.empty((batch_size, im_size, im_size, 3))\n",
    "            labels = np.empty((batch_size, im_size, im_size, num_classes))\n",
    "            for j, img in enumerate(imgs[i:i+batch_size]):\n",
    "                # Crop the size we want from a random spot in the image (as a form of\n",
    "                # minor data augmentation)\n",
    "                new_start_row = np.random.randint(0, 1024 - im_size)\n",
    "                new_start_col = np.random.randint(0, 2048 - im_size)\n",
    "                train_im = misc.imread(img).astype(np.float32)\n",
    "                \n",
    "                train_im = train_im[new_start_row:new_start_row+im_size, new_start_col:new_start_col+im_size]\n",
    "                images[j,:,:,:] = train_im\n",
    "                \n",
    "                gt_im = misc.imread(gt[i+j])\n",
    "                gt_im = gt_im[new_start_row:new_start_row+im_size, new_start_col:new_start_col+im_size]\n",
    "                labels[j,:,:,:] = prepare_ground_truth(gt_im)\n",
    "            \n",
    "            yield(images,labels)\n",
    " \n",
    "    '''\n",
    "    for i in range(0,len(imgs),batch_size):\n",
    "        #images = np.empty((batch_size, im_size, im_size, shape_im.shape[2]))\n",
    "        images = np.empty((batch_size, im_size, im_size, 3))\n",
    "        labels = np.empty((batch_size, im_size, im_size, num_classes))\n",
    "        for j, img in enumerate(imgs[i:i+batch_size]):\n",
    "            # Crop the size we want from a random spot in the image (as a form of\n",
    "            # minor data augmentation)\n",
    "            new_start_row = np.random.randint(0, 1024 - im_size)\n",
    "            new_start_col = np.random.randint(0, 2048 - im_size)\n",
    "            train_im = misc.imread(img).astype(np.float32)\n",
    "            \n",
    "            train_im = train_im[new_start_row:new_start_row+im_size, new_start_col:new_start_col+im_size]\n",
    "            images[j,:,:,:] = train_im\n",
    "            \n",
    "            gt_im = misc.imread(gt[i+j])\n",
    "            gt_im = gt_im[new_start_row:new_start_row+im_size, new_start_col:new_start_col+im_size]\n",
    "            labels[j,:,:,:] = prepare_ground_truth(gt_im)\n",
    "        \n",
    "        yield(images,labels)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize_prediction for cityscape data\n",
    "def visualize_prediction(original_image, prediction):\n",
    "    \n",
    "    original_image = np.squeeze(original_image).astype(np.uint8)\n",
    "    new_image = np.copy(original_image)\n",
    "    prediction = np.squeeze(prediction)\n",
    "    mask = np.argmax(prediction, axis=2)\n",
    "    # road = green\n",
    "    new_image[mask[:,:]==0, :] = [0,255,0]\n",
    "    # sidewalk = blue\n",
    "    new_image[mask[:,:]==1, :] = [0,0,255]\n",
    "    # pedestrians = yellow\n",
    "    new_image[mask[:,:]==2, :] = [255,255,0]\n",
    "    # vehicles = red\n",
    "    new_image[mask[:,:]==3, :] = [255,0,0]\n",
    "    # else is left the same\n",
    "    \n",
    "    new_image = Image.blend(Image.fromarray(original_image, mode='RGB').convert('RGBA'),\n",
    "                            Image.fromarray(new_image, mode='RGB').convert('RGBA'),\n",
    "                            alpha=0.5)\n",
    "    \n",
    "    plt.imshow(new_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_conv(layer_input, vgg_dict, bn, name):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # Get the conv filter\n",
    "        init = tf.constant_initializer(value=vgg_dict[name][0], dtype=tf.float32)\n",
    "        shape = vgg_dict[name][0].shape\n",
    "        filt = tf.get_variable(name=\"filter\", initializer=init, shape=shape)\n",
    "        # Get the conv bias\n",
    "        init = tf.constant_initializer(value=vgg_dict[name][1], dtype=tf.float32)\n",
    "        shape = vgg_dict[name][1].shape\n",
    "        bias = tf.get_variable(name=\"biases\", initializer=init, shape=shape)\n",
    "        # Construct conv layer\n",
    "        conv = tf.nn.conv2d(layer_input, filt, [1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, bias), name=name)\n",
    "        return tf.contrib.layers.batch_norm(relu, center=True, scale=True, is_training=True)\n",
    "    \n",
    "# Helper function for deconvolutional layers\n",
    "# Note that this function is different than the Unet deconv functions\n",
    "def deconv2d(layer_input, output_shape, input_fsize, output_fsize, filter_size, stride, bn, name):\n",
    "    w = tf.get_variable(\"filter_\" + name, shape=[filter_size, filter_size, output_fsize, input_fsize], \n",
    "                        initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                        regularizer=tf.contrib.layers.l2_regularizer(.001))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[output_fsize]))\n",
    "    new_shape = tf.stack([output_shape[0], output_shape[1], output_shape[2], output_fsize])\n",
    "    deconv = tf.nn.conv2d_transpose(layer_input, w, new_shape, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    layer = tf.nn.elu(tf.add(deconv, b))\n",
    "    return tf.contrib.layers.batch_norm(layer, center=True, scale=True, is_training=True)\n",
    "\n",
    "def conv_layer(layer_input, input_fsize, output_fsize, filter_size, bn, name):\n",
    "    w = tf.get_variable(\"filter_\" + name, shape=[filter_size, filter_size, input_fsize, output_fsize], \n",
    "                        initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                        regularizer=tf.contrib.layers.l2_regularizer(.001))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[output_fsize]))\n",
    "    layer = tf.nn.elu(tf.add(tf.nn.conv2d(layer_input, w, strides=[1, 1, 1, 1], padding='SAME'), b))\n",
    "    return tf.contrib.layers.batch_norm(layer, center=True, scale=True, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_FCN_VGG(imgs, vgg_dict, dropout=0.5, channels=3, num_classes=5, filter_size=1, bn=True):\n",
    "    \n",
    "    im_rows = tf.shape(imgs)[1]\n",
    "    im_cols = tf.shape(imgs)[2]\n",
    "    \n",
    "    x_image = tf.reshape(imgs, tf.stack([-1,im_rows,im_cols,channels]))\n",
    "    layer_input = x_image\n",
    "    batch_size = tf.shape(x_image)[0]\n",
    "    \n",
    "    # VGG pre-processing\n",
    "    red, green, blue = tf.split(x_image, 3, 3)\n",
    "\n",
    "    # From the VGG paper\n",
    "    vgg_means = [103.939, 116.779, 123.68]\n",
    "    x_image = tf.concat([\n",
    "        blue - vgg_means[0],\n",
    "        green - vgg_means[1],\n",
    "        red - vgg_means[2]], axis=3)\n",
    "    \n",
    "    x_image = x_image / 122.5\n",
    "    \n",
    "    # Encoder\n",
    "    conv1_1 = vgg_conv(x_image, vgg_dict, bn, \"conv1_1\")\n",
    "    conv1_2 = vgg_conv(conv1_1, vgg_dict, bn, \"conv1_2\")\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME', name=\"pool1\")\n",
    "    conv2_1 = vgg_conv(pool1, vgg_dict, bn, \"conv2_1\")\n",
    "    conv2_2 = vgg_conv(conv2_1, vgg_dict, bn, \"conv2_2\")\n",
    "    pool2 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME', name=\"pool2\")\n",
    "    conv3_1 = vgg_conv(pool2, vgg_dict, bn, \"conv3_1\")\n",
    "    conv3_2 = vgg_conv(conv3_1, vgg_dict, bn, \"conv3_2\")\n",
    "    conv3_3 = vgg_conv(conv3_2, vgg_dict, bn, \"conv3_3\")\n",
    "    pool3 = tf.nn.max_pool(conv3_3, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME', name=\"pool3\")\n",
    "    conv4_1 = vgg_conv(pool3, vgg_dict, bn, \"conv4_1\")\n",
    "    conv4_2 = vgg_conv(conv4_1, vgg_dict, bn, \"conv4_2\")\n",
    "    conv4_3 = vgg_conv(conv4_2, vgg_dict, bn, \"conv4_3\")\n",
    "    pool4 = tf.nn.max_pool(conv4_3, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME', name=\"pool4\")\n",
    "    conv5_1 = vgg_conv(pool4, vgg_dict, bn, \"conv5_1\")\n",
    "    conv5_2 = vgg_conv(conv5_1, vgg_dict, bn, \"conv5_2\")\n",
    "    conv5_3 = vgg_conv(conv5_2, vgg_dict, bn, \"conv5_3\")\n",
    "    pool5 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME', name=\"pool5\")\n",
    "\n",
    "    # VGG depth at the fifth layer (comes from imported weights so\n",
    "    # it will be constant no matter the size of input image)\n",
    "    features = 512\n",
    "        \n",
    "    # Decoder\n",
    "    # Combo layer 1\n",
    "    d_conv1 = conv_layer(pool5, features, num_classes, filter_size, bn, \"decoder_c1\")\n",
    "    d_dconv1 = deconv2d(d_conv1, tf.shape(pool4), num_classes, num_classes, 4, 2, bn, \"decoder_d1\")\n",
    "    \n",
    "    d_conv2 = conv_layer(pool4, features, num_classes, filter_size, bn, \"decoder_c2\")\n",
    "    \n",
    "    # Skip connection\n",
    "    d_sum1 = tf.add(d_dconv1, d_conv2)\n",
    "    \n",
    "    # Combo layer 2\n",
    "    d_dconv2 = deconv2d(d_sum1, tf.shape(pool3), num_classes, num_classes, 4, 2, bn, \"decoder_d2\")\n",
    "    \n",
    "    d_conv3 = conv_layer(pool3, features//2, num_classes, filter_size, bn, \"decoder_c3\")\n",
    "    \n",
    "    # Skip connection\n",
    "    d_sum2 = tf.add(d_dconv2, d_conv3)\n",
    "    \n",
    "    # Combo layer 3\n",
    "    d_dconv3 = deconv2d(d_sum2, tf.shape(x_image), num_classes, num_classes, 16, 8, bn, \"decoder_d3\")\n",
    "\n",
    "    output = d_dconv3\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FCN_VGG:\n",
    "    \n",
    "    def __init__(self, channels=3, num_classes=5, output_path='./output'):\n",
    "        \n",
    "        # prediction path is to store validation prediction images in if necessary\n",
    "        self.prediction_path = \"./predictions\"\n",
    "        \n",
    "        # output path for trained model \n",
    "        self.output_path = output_path\n",
    "        \n",
    "        if not os.path.exists(self.prediction_path):\n",
    "            os.mkdir(self.prediction_path)\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.mkdir(self.output_path)\n",
    "                \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # load the VGG weights : vgg16.npy\n",
    "        # downloaded into current directory from ftp://mi.eng.cam.ac.uk/pub/mttt2/models/vgg16.npy\n",
    "        vgg_path = 'vgg16.npy'\n",
    "        self.vgg_dict = np.load(vgg_path, encoding='latin1').item()\n",
    "        \n",
    "        # images as input\n",
    "        self.x = tf.placeholder(\"float\", shape=[None, None, None, channels], name=\"x\")\n",
    "        \n",
    "        # prediction output\n",
    "        self.y = tf.placeholder(\"float\", shape=[None, None, None, num_classes], name=\"y\")\n",
    "        \n",
    "        # dropout prob.\n",
    "        self.drop_pl = tf.placeholder(\"float\", name=\"do\")\n",
    "        \n",
    "        # batch norm\n",
    "        self.bn = tf.placeholder(\"bool\", name=\"bn\")\n",
    "        \n",
    "        # output logit\n",
    "        logits = create_FCN_VGG(self.x, self.vgg_dict, dropout=self.drop_pl, channels=channels, num_classes=num_classes)\n",
    "        self.logits = logits\n",
    "        \n",
    "        # convert to prob. using softmax\n",
    "        self.soft = self.pixel_wise_softmax(logits)\n",
    "        self.soft = tf.identity(self.soft, name=\"output\")\n",
    "        self.loss = self.get_loss(logits)\n",
    "                \n",
    "        # These two are for validation\n",
    "        self.correct_pred = tf.equal(tf.argmax(tf.squeeze(self.soft), -1), tf.argmax(tf.squeeze(self.y), -1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "            \n",
    "    # A helper function for computing softmax across the channel-dimension\n",
    "    def pixel_wise_softmax(self, output):\n",
    "                \n",
    "        # Softmax across the last dimension (where each channel is a binary image\n",
    "        # denoting whether the pixel belongs in that category or not -- this allows\n",
    "        # for more than two classes)\n",
    "        # e^x\n",
    "        \n",
    "        # output is logits\n",
    "        exponential_map = tf.exp(output)\n",
    "        # sum-e^x\n",
    "        sum_exp = tf.reduce_sum(exponential_map, 3, keep_dims=True)\n",
    "        # duplicate the last summed dimension\n",
    "        tensor_sum_exp = tf.tile(sum_exp, tf.stack([1, 1, 1, tf.shape(output)[3]]))\n",
    "        # divide e^x by sum-e^x \n",
    "        return tf.div(exponential_map, tensor_sum_exp)\n",
    "        \n",
    "    def get_loss(self, logits):\n",
    "\n",
    "        cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=logits))\n",
    "        \n",
    "        reg_loss = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "        \n",
    "        cost += reg_loss\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def predict(self, model_path, img, gt=None, restore=True):\n",
    "    \n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(init)\n",
    "            # model_path = 'fcn_vgg_city2.ckpt'\n",
    "            if restore:\n",
    "                self.restore(sess, os.path.join(self.output_path, model_path))\n",
    "            \n",
    "            if gt != None:\n",
    "                soft, acc = sess.run([self.soft, self.accuracy], feed_dict={self.x: img, self.y: gt,\n",
    "                                                                            self.drop_pl: 1.0, self.bn: False})\n",
    "                return soft, acc\n",
    "            else:\n",
    "                soft = sess.run(self.soft, feed_dict={self.x: img, self.drop_pl: 1.0, self.bn: False})\n",
    "                return soft\n",
    "    \n",
    "    def save(self, sess, model_path):\n",
    "        # save the variabless in sess (including weights and model structures) into model_path ('fcn_vgg_city2.ckpt')\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        return save_path\n",
    "    \n",
    "    def restore(self, sess, model_path):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "    \n",
    "    def train(self, data_generator, dropout=0.5, training_iters=10, learning_rate=0.0001, epochs=10, display_step=10, restore=False, model_path=None):\n",
    "        # output_path = './output', model_path = 'fcn_vgg_city2.ckpt'\n",
    "        # then model_path = './output/fcn_vgg_city2.ckpt'\n",
    "        model_path = os.path.join(self.output_path, model_path)\n",
    "                \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            print(\"Session begun\")\n",
    "            \n",
    "            sess.run(init)\n",
    "        \n",
    "            if restore:\n",
    "                self.restore(sess, model_path)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                print(\"Starting new epoch\")\n",
    "                \n",
    "                total_loss = 0.0\n",
    "                display_loss = 0.0\n",
    "                display_acc = 0.0\n",
    "                best_acc = .93\n",
    "                for step in range((epoch*training_iters), ((epoch+1)*training_iters)):\n",
    "                    \n",
    "                    x_batch, y_batch = next(data_generator)\n",
    "                    \n",
    "                    _, loss, acc, logits = sess.run((self.optimizer, self.loss, self.accuracy, self.logits),\n",
    "                                                    feed_dict={self.x: x_batch,\n",
    "                                                              self.y: y_batch,\n",
    "                                                              self.drop_pl: dropout,\n",
    "                                                              self.bn: True})\n",
    "                    \n",
    "                    display_loss += loss\n",
    "                    display_acc += acc\n",
    "                    \n",
    "                    if ((step+1) % display_step == 0):\n",
    "                        print(\"At iteration {} loss equals {} and accuracy equals {}\".format(step+1, display_loss/(display_step), display_acc/(display_step)))\n",
    "                        \n",
    "                        if ((display_acc/(display_step)) > best_acc):\n",
    "                            save_path = self.save(sess, os.path.join(self.output_path, model_path))\n",
    "                            best_acc = display_acc/display_step\n",
    "                            print(\"model saved\")\n",
    "                        total_loss += display_loss\n",
    "                        display_loss = 0.0\n",
    "                        display_acc = 0.0\n",
    "                        \n",
    "                                                                    \n",
    "                print(\"###############################\")\n",
    "                print(\"Epoch\", epoch + 1, \"average loss =\", total_loss/training_iters)\n",
    "                print(\"###############################\")\n",
    "                total_loss = 0                \n",
    "                \n",
    "            #save_path = self.save(sess, os.path.join(self.output_path, model_path))\n",
    "            save_path = self.save(sess, os.path.join(model_path))\n",
    "        \n",
    "        return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "my_fcn = FCN_VGG()\n",
    "\n",
    "# Not enough RAM for large batch given\n",
    "# the size of these images\n",
    "batch_generator = get_data(mode='train', batch_size=1, im_size=500)\n",
    "#batch_generator = get_data(mode='val', batch_size=10, im_size=500)\n",
    "model_path = 'fcn_vgg_city3.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_path = my_fcn.train(batch_generator, learning_rate=8e-4,\n",
    "                          training_iters=200, epochs=1, display_step=100,\n",
    "                          restore=False, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
